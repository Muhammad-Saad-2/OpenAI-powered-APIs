{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "#Load environment variables from .env file \n",
    "_ : bool = load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI = OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* First Response:  {'content': None, 'role': 'assistant', 'function_call': None, 'tool_calls': [ChatCompletionMessageToolCall(id='call_iwWszGrrAswHrDhAM78Gf4YV', function=Function(arguments='{\"animal\": \"dog\"}', name='get_animal_info'), type='function')]}\n",
      "* First Reponse Tool Calls:  [ChatCompletionMessageToolCall(id='call_iwWszGrrAswHrDhAM78Gf4YV', function=Function(arguments='{\"animal\": \"dog\"}', name='get_animal_info'), type='function')]\n",
      "* Second Request Messages:  [{'role': 'user', 'content': 'Tell me about dogs.'}, ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_iwWszGrrAswHrDhAM78Gf4YV', function=Function(arguments='{\"animal\": \"dog\"}', name='get_animal_info'), type='function')]), {'tool_call_id': 'call_iwWszGrrAswHrDhAM78Gf4YV', 'role': 'tool', 'name': 'get_animal_info', 'content': '{\"animal\": \"dog\", \"info\": \"Dogs are loyal companions.\"}'}]\n",
      "* Second Response:  {'id': 'chatcmpl-8i0kz9u9ggOxUXlpQFy2f4EYaUyxA', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Dogs are loyal companions and are known for their friendly and loving nature. They come in a wide variety of breeds, each with its own unique characteristics and traits. Dogs are highly social animals and tend to form strong bonds with their human families. They are also known for their intelligence and ability to be trained for various tasks such as herding, hunting, and providing assistance to people with disabilities. Overall, dogs make wonderful pets and bring joy and companionship to the lives of their owners.', role='assistant', function_call=None, tool_calls=None))], 'created': 1705500033, 'model': 'gpt-3.5-turbo-1106', 'object': 'chat.completion', 'system_fingerprint': 'fp_fe56e538d5', 'usage': CompletionUsage(completion_tokens=98, prompt_tokens=55, total_tokens=153)}\n",
      "Final Result: Dogs are loyal companions and are known for their friendly and loving nature. They come in a wide variety of breeds, each with its own unique characteristics and traits. Dogs are highly social animals and tend to form strong bonds with their human families. They are also known for their intelligence and ability to be trained for various tasks such as herding, hunting, and providing assistance to people with disabilities. Overall, dogs make wonderful pets and bring joy and companionship to the lives of their owners.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ : bool = load_dotenv(find_dotenv())\n",
    "client : OpenAI = OpenAI()\n",
    "\n",
    "# Example dummy function for animal information\n",
    "def get_animal_info(animal: str) -> str:\n",
    "    \"\"\"Get information about a specific animal\"\"\"\n",
    "    # Define information about different animals\n",
    "    animal_info = {\n",
    "        \"dog\": \"Dogs are loyal companions.\",\n",
    "        \"cat\": \"Cats are independent and make great pets.\",\n",
    "        \"bird\": \"Birds come in various species and are known for their songs.\",\n",
    "    }\n",
    "    # Return information about the specified animal or \"Unknown animal\" if not found\n",
    "    return json.dumps({\"animal\": animal, \"info\": animal_info.get(animal, \"Unknown animal\")})\n",
    "\n",
    "from openai.types.chat.chat_completion import ChatCompletionMessage, ChatCompletion\n",
    "\n",
    "\n",
    "def run_animal_conversation(main_request: str) -> str:\n",
    "    # Step 1: Create a list of messages, starting with the user's prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": main_request}]\n",
    "    \n",
    "    # Step 1.1: Define a tool (function) that the model can use during the conversation\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_animal_info\",\n",
    "                \"description\": \"Get information about a specific animal\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"animal\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The name of the animal\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"animal\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First Request\n",
    "    # Step 2: Send the user's prompt and available functions to the model\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is the default, but we'll be explicit\n",
    "    )\n",
    "    response_message: ChatCompletionMessage = response.choices[0].message\n",
    "    \n",
    "    # Display the assistant's first response\n",
    "    print(\"* First Response: \", dict(response_message))\n",
    "\n",
    "    tool_calls = response_message.tool_calls\n",
    "    \n",
    "    # Display tool calls made by the assistant in the first response\n",
    "    print(\"* First Reponse Tool Calls: \", list(tool_calls))\n",
    "\n",
    "    # Step 2.1: Check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: Call the function\n",
    "        available_functions = {\n",
    "            \"get_animal_info\": get_animal_info,\n",
    "        }\n",
    "\n",
    "        messages.append(response_message)  # Extend conversation with the assistant's reply\n",
    "\n",
    "        # Step 4: Send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            \n",
    "            # Call the function and get its response\n",
    "            function_response = function_to_call(\n",
    "                animal=function_args.get(\"animal\"),\n",
    "            )\n",
    "            \n",
    "            # Extend conversation with the function response\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  \n",
    "\n",
    "        # Display the messages sent in the second request\n",
    "        print(\"* Second Request Messages: \", list(messages))\n",
    "        \n",
    "        # Step 5: Get a new response from the model where it can see the function response\n",
    "        second_response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        \n",
    "        # Display the assistant's second response\n",
    "        print(\"* Second Response: \", dict(second_response))\n",
    "        return second_response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "user_question = \"Tell me about dogs.\"\n",
    "result = run_animal_conversation(user_question)\n",
    "print(\"Final Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ : bool = load_dotenv (find_dotenv())\n",
    "\n",
    "client : OpenAI = OpenAI()\n",
    "\n",
    "# Import the 'json' module for working with JSON data\n",
    "import json\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "\n",
    "def get_current_weather(location: str, unit: str = \"fahrenheit\") -> str:\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    \n",
    "    print(\"Location:\" , location)\n",
    "    \n",
    "    # Convert the location to lowercase for case-insensitive comparison\n",
    "    location_lower = location.lower()\n",
    "\n",
    "    # Check if the location is \"tokyo\"\n",
    "    if \"tokyo\" in location_lower:\n",
    "        # If it's Tokyo, return JSON-encoded weather data for Tokyo\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    \n",
    "    # Check if the location is \"san francisco\"\n",
    "    elif \"san francisco\" in location_lower:\n",
    "        # If it's San Francisco, return JSON-encoded weather data for San Francisco\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    \n",
    "    # Check if the location is \"paris\"\n",
    "    elif \"paris\" in location_lower:\n",
    "        # If it's Paris, return JSON-encoded weather data for Paris\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    \n",
    "    # If the location doesn't match any predefined locations\n",
    "    else:\n",
    "        # Return JSON-encoded weather data with \"unknown\" temperature for the provided location\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletionMessage, ChatCompletion\n",
    "# from openai.types.chat.chat_completion import ChatCompletionMessageParam, ChatCompletionMessageParam\n",
    "\n",
    "def run_conversation(main_request: str)->str:\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": main_request}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First Request\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message: ChatCompletionMessage = response.choices[0].message\n",
    "    display(\"* First Response: \", dict(response_message))\n",
    "\n",
    "    tool_calls = response_message.tool_calls\n",
    "    display(\"* First Reponse Tool Calls: \", list(tool_calls))\n",
    "\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        \n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        \n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        display(\"* Second Request Messages: \", list(messages))\n",
    "        second_response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        print(\"* Second Response: \", dict(second_response))\n",
    "        return second_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* First Response: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'content': None,\n",
       " 'role': 'assistant',\n",
       " 'function_call': None,\n",
       " 'tool_calls': [ChatCompletionMessageToolCall(id='call_9VMKdobNJqb7V2t4o2en0qxz', function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       "  ChatCompletionMessageToolCall(id='call_ruvEg8j1Uuea2jq1anei8Azw', function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       "  ChatCompletionMessageToolCall(id='call_fBZ4MOGboeCQrln97x8ZPVhz', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* First Reponse Tool Calls: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_9VMKdobNJqb7V2t4o2en0qxz', function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_ruvEg8j1Uuea2jq1anei8Azw', function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_fBZ4MOGboeCQrln97x8ZPVhz', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: San Francisco\n",
      "Location: Tokyo\n",
      "Location: Paris\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'* Second Request Messages: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'what is the current weather in San Francisco, Tokyo and Paris ?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9VMKdobNJqb7V2t4o2en0qxz', function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_ruvEg8j1Uuea2jq1anei8Azw', function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_fBZ4MOGboeCQrln97x8ZPVhz', function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]),\n",
       " {'tool_call_id': 'call_9VMKdobNJqb7V2t4o2en0qxz',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}'},\n",
       " {'tool_call_id': 'call_ruvEg8j1Uuea2jq1anei8Azw',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}'},\n",
       " {'tool_call_id': 'call_fBZ4MOGboeCQrln97x8ZPVhz',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Second Response:  {'id': 'chatcmpl-8hbtsEkR2IcQyXOW2Ry8tFzSp0cy8', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current weather in San Francisco is 72°F, in Tokyo it is 10°C, and in Paris it is 22°C.', role='assistant', function_call=None, tool_calls=None))], 'created': 1705404484, 'model': 'gpt-3.5-turbo-1106', 'object': 'chat.completion', 'system_fingerprint': 'fp_cbe4fa03fe', 'usage': CompletionUsage(completion_tokens=28, prompt_tokens=168, total_tokens=196)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current weather in San Francisco is 72°F, in Tokyo it is 10°C, and in Paris it is 22°C.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conversation(\"what is the current weather in San Francisco, Tokyo and Paris ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src = \"https://raw.githubusercontent.com/panaverse/learn-generative-ai/28b9b98f626f9bea460bd81b86aea3e0355332fe/03_chatgpt/05_function_calling/second.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "#Load environment variables from .env file \n",
    "_ : bool = load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI = OpenAI()\n",
    "\n",
    "def weather_function (location : str, unit : str = \"fahrenheit\")->str:\n",
    "    if \"Tokyo\" in location:\n",
    "        return json.dumps ({\"location\": \"Tokyo\", \"temperature\": 22, \"unit\": \"celicius\"})\n",
    "    elif \"California\" in location:\n",
    "        return json.dumps({\"location\": \"California\", \"temperature\": 55, \"unit\": \"fahrenheit\"})\n",
    "    elif \"San Francisco\" in location:\n",
    "        return json.dumps ({\"location\": \"San Francisco\", \"tempearature\": 75, \"unit\": \"celcius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"I'm sorry I can't tell you\"})\n",
    "    \n",
    "from openai.types.chat.chat_completion import ChatCompletionMessage, ChatCompletion\n",
    "\n",
    "def conversation (user_request : str)->str:\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\" : user_request}]\n",
    "\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\":{\n",
    "                \"name\": \"weather_function\",\n",
    "                \"description\": \"get the current weathher of the given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\":\"string\",\n",
    "                            \"description\": \"Name of the location or the state like California or San Francisco et cetra\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\":[\"celcius\",\"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response : ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\", #auto is default, like to make the function run, we will write Auto, if you write none the function wom't run \n",
    "    )\n",
    "\n",
    "    response_message: ChatCompletion = response.choices[0].message   # reponse_message kai naam se variable create kar kai pehla response print karwaya \n",
    "    print(\"First Response:\", dict(response_message))\n",
    "\n",
    "    tool_calls = response_message.tool_calls                      #tool_calls ka variable create kiya, us ko first response se connect kiya and pehla tool call ka respone print krwaya \n",
    "    print(\"* First response tool calls:\", list(tool_calls))\n",
    "\n",
    "    if tool_calls:\n",
    "        available_functions = {\n",
    "            \"weather_function\": weather_function,\n",
    "        }\n",
    "\n",
    "        messages.append(response_message)\n",
    "\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location = function_args.get(\"location\"),\n",
    "                unit = function_args.get(\"unit\"),\n",
    "            )\n",
    "\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "\n",
    "     \n",
    "            display(\"Second Request Message:\", list(messages))\n",
    "            second_response: ChatCompletion = client.chat.completions.create(\n",
    "                model = \"gpt-3.5-turbo-1106\",\n",
    "                messages = messages,\n",
    "            )\n",
    "\n",
    "            print (\"Second Response:\", dict(second_response))\n",
    "            return second_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Response: {'content': None, 'role': 'assistant', 'function_call': None, 'tool_calls': [ChatCompletionMessageToolCall(id='call_rdfWrujdmvon8kcgTxWDCsbl', function=Function(arguments='{\"location\": \"California\", \"unit\": \"celcius\"}', name='weather_function'), type='function')]}\n",
      "* First response tool calls: [ChatCompletionMessageToolCall(id='call_rdfWrujdmvon8kcgTxWDCsbl', function=Function(arguments='{\"location\": \"California\", \"unit\": \"celcius\"}', name='weather_function'), type='function')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Second Request Message:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the weather in California'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_rdfWrujdmvon8kcgTxWDCsbl', function=Function(arguments='{\"location\": \"California\", \"unit\": \"celcius\"}', name='weather_function'), type='function')]),\n",
       " {'tool_call_id': 'call_rdfWrujdmvon8kcgTxWDCsbl',\n",
       "  'role': 'tool',\n",
       "  'name': 'weather_function',\n",
       "  'content': '{\"location\": \"California\", \"temperature\": 55, \"unit\": \"fahrenheit\"}'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Response: {'id': 'chatcmpl-8iaZSOAxUJsQ0pMYVgtM5Y8OoJ0Ds', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current temperature in California is 55°F.', role='assistant', function_call=None, tool_calls=None))], 'created': 1705637702, 'model': 'gpt-3.5-turbo-1106', 'object': 'chat.completion', 'system_fingerprint': 'fp_c596c86df9', 'usage': CompletionUsage(completion_tokens=10, prompt_tokens=60, total_tokens=70)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current temperature in California is 55°F.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation(\"What is the weather in California\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
